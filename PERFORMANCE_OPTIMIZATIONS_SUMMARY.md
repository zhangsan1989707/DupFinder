# 文本查重性能优化总结

## 问题分析

原始问题：3.1G，635个文件，花费6分钟且在"正在预处理文本"阶段卡死

### 根本原因
1. **内存爆炸**：同时加载所有文件内容到内存（3.1G数据）
2. **重复I/O**：文件被多次读取（扫描时计算哈希，预处理时再次读取）
3. **O(n²)复杂度**：635个文件需要约20万次相似度比较
4. **低效特征提取**：每个文件都要提取复杂的文本特征

## 优化方案

### 1. 延迟加载策略
- **扫描阶段**：只获取基本文件信息（路径、大小、修改时间）
- **按需计算**：编码检测、哈希计算、统计信息只在需要时进行
- **内存控制**：避免同时加载大量文件内容

### 2. 分层过滤机制
- **第一层**：文件大小预过滤（±20%范围内才比较）
- **第二层**：轻量级特征快速筛选
- **第三层**：详细相似度计算（只对候选对进行）

### 3. 流式处理优化
- **分块读取**：大文件分块处理，避免内存溢出
- **采样比较**：长文本使用采样算法而非完整比较
- **批处理**：文件分批处理，控制内存使用

### 4. 算法优化
- **快速预筛选**：基于首尾行哈希、词汇重叠等快速判断
- **智能采样**：对大文件从开头、中间、结尾取样比较
- **并行计算**：多线程处理相似度计算

## 具体实现

### 扫描器优化 (text_scanner.py)
```python
# 延迟加载文件详细信息
def _get_file_info(self, file_path: str):
    return {
        'path': file_path,
        'size': file_size,
        'mtime': stat.st_mtime,
        # 延迟计算的字段
        'encoding': None,
        'content_hash': None
    }

# 按需加载详细信息
def ensure_file_details(self, file_info: Dict[str, Any]):
    # 只在需要时计算编码、哈希等
```

### 检测器优化 (text_duplicate_detector.py)
```python
# 分层过滤
def _pre_filter_files(self, text_files):
    # 按文件大小分组，只比较相近大小的文件

def _fast_pre_screening(self, files):
    # 轻量级特征快速筛选

def _detailed_similarity_check(self, candidate_pairs):
    # 只对候选对进行详细计算
```

### 性能配置
```python
self.config = {
    'batch_size': 100,          # 批处理大小
    'max_content_size': 50000,  # 最大内容读取大小
    'quick_check_threshold': 0.3, # 快速检查阈值
    'sample_positions': 5,      # 采样位置数量
}
```

## 预期性能提升

### 内存使用
- **优化前**：3.1G+ 内存（所有文件内容同时加载）
- **优化后**：<100MB 内存（流式处理 + 批处理）

### 处理时间
- **优化前**：6分钟+（经常卡死）
- **优化后**：预计30-60秒（基于算法复杂度降低）

### 处理能力
- **优化前**：~8.6 MB/分钟
- **优化后**：预计100+ MB/分钟

## 使用方法

### 运行性能测试
```bash
python test_performance_optimized.py test_texts
```

### 配置调优
根据硬件配置调整参数：
- `max_workers`：CPU核心数的1-2倍
- `batch_size`：内存大小决定（8GB内存建议100）
- `max_content_size`：单文件最大读取量

## 进度反馈优化

新增详细的进度报告：
- 扫描阶段：显示文件数量和处理速度
- 预过滤：显示过滤后的文件数量
- 批处理：显示当前批次和候选对数量
- 完成时：显示总用时和处理速度

## 错误处理改进

- 单个文件处理失败不影响整体流程
- 批处理失败时跳过该批次继续处理
- 详细的错误日志和恢复机制

## 验证方法

1. **功能验证**：确保优化后结果准确性不变
2. **性能验证**：使用 `test_performance_optimized.py` 测试
3. **内存监控**：观察内存使用情况
4. **压力测试**：测试更大数据集的处理能力

## 后续优化建议

1. **缓存机制**：文件哈希结果缓存
2. **增量处理**：只处理新增/修改的文件
3. **GPU加速**：使用GPU进行相似度计算
4. **分布式处理**：多机器并行处理大数据集